{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "finetuning_torchvision_models_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79731c780f194ed7827d9eccd35c0aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e69d6da97b23426295bf5f8dc1c9a80d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_082051845fd14a75a14961e05606d8f5",
              "IPY_MODEL_cf9cdb28b9ee44f2a87afb9dd414cb77",
              "IPY_MODEL_2ec5eb70fa284210acc8b828dd6fd394"
            ]
          }
        },
        "e69d6da97b23426295bf5f8dc1c9a80d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "082051845fd14a75a14961e05606d8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8ac63f248527417d8c7a3b21669b727e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd0473e7a5294cd0a6d7bca55e29142a"
          }
        },
        "cf9cdb28b9ee44f2a87afb9dd414cb77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7d73b24850a8451892d594b3ed34eabe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5010551,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5010551,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca3f8de0388445feae00f2b0f50759d0"
          }
        },
        "2ec5eb70fa284210acc8b828dd6fd394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a847ae59e16542e9a195ef7a127bb8cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.78M/4.78M [00:00&lt;00:00, 21.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28c6b88904c445c78ee2319926dc7c5b"
          }
        },
        "8ac63f248527417d8c7a3b21669b727e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd0473e7a5294cd0a6d7bca55e29142a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d73b24850a8451892d594b3ed34eabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca3f8de0388445feae00f2b0f50759d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a847ae59e16542e9a195ef7a127bb8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28c6b88904c445c78ee2319926dc7c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SauloHenriqueAguiar/DeepLearningCNN/blob/main/torchvisionmodelscompile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IpDiTahhstZ"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q66F1M1Xhsth"
      },
      "source": [
        "Neste tutorial, daremos uma olhada em como ajustar e extrair recursos dos modelos torchvision, todos os quais foram pré-treinados no conjunto de dados Imagenet de 1000 classes. Este tutorial dará uma visão detalhada de como trabalhar com várias arquiteturas CNN modernas e criará uma intuição para ajustar qualquer modelo PyTorch. Como cada arquitetura de modelo é diferente, não existe um código de ajuste fino padrão que funcione em todos os cenários. Em vez disso, o pesquisador deve olhar para a arquitetura existente e fazer ajustes personalizados para cada modelo.\n",
        "\n",
        "Neste documento, realizaremos dois tipos de aprendizagem por transferência: ajuste fino e extração de recursos. No ajuste fino, começamos com um modelo pré-treinado e atualizamos todos os parâmetros do modelo para nossa nova tarefa, em essência, retreinamos todo o modelo. Na extração de recursos, começamos com um modelo pré-treinado e apenas atualizamos os pesos das camadas finais das quais derivamos as previsões. É chamado de extração de recursos porque usamos o CNN pré-treinado como um extrator de recursos fixo e apenas alteramos a camada de saída. Para obter mais informações técnicas sobre transferência de aprendizagem.\n",
        "\n",
        "Em geral, os dois métodos de aprendizagem por transferência seguem as mesmas etapas:\n",
        "\n",
        "-  Inicialize o modelo pré-treinado.\n",
        "- Remodele a (s) camada (s) final (is) para ter o mesmo número de saídas que o número de classes no novo conjunto de dados.\n",
        "-  Defina para o algoritmo de otimização quais parâmetros queremos atualizar durante o treinamento.\n",
        "-  Execute a etapa de treinamento.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpS6zX36hsti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f984585b-2eec-424b-b7c7-4a04b81a6ef3"
      },
      "source": [
        "from __future__ import print_function \n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7FLIYWthstl"
      },
      "source": [
        "Inputs\n",
        "------\n",
        "\n",
        "Aqui estão todos os parâmetros a serem alterados durante a execução. Usaremos o conjunto de dados hymenoptera_data que pode ser baixado aqui. Este conjunto de dados contém duas classes, abelhas e formigas, e é estruturado de forma que possamos usar o conjunto de dados ImageFolder, em vez de escrever nosso próprio conjunto de dados personalizado. Baixe os dados e defina a entrada data_dir para o diretório raiz do conjunto de dados. A entrada model_name é o nome do modelo que você deseja usar e deve ser selecionado nesta lista:\n",
        "\n",
        "[resnet, alexnet, vgg, squeezenet, densenet, início]\n",
        "As outras entradas são as seguintes: num_classes é o número de classes no conjunto de dados, batch_size é o tamanho do lote usado para treinamento e pode ser ajustado de acordo com a capacidade de sua máquina, num_epochs é o número de épocas de treinamento que queremos executar e feature_extract é um booleano que define se estamos ajustando ou extraindo recursos. Se feature_extract = False, o modelo é ajustado e todos os parâmetros do modelo são atualizados. Se feature_extract = True, apenas os parâmetros da última camada são atualizados, os outros permanecem fixos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczY4hWohstm"
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms \n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"./hymenoptera_data\"\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"squeezenet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 15\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model, \n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLlkhrgKhstp"
      },
      "source": [
        "Funções auxiliares\n",
        "----------------\n",
        "\n",
        "Antes de escrevermos o código para ajustar os modelos, vamos definir algumas funções auxiliares.\n",
        "\n",
        "Treinando modelo e Validando Codigo\n",
        "----------------------------------\n",
        "\n",
        "A função train_model lida com o treinamento e validação de um determinado modelo. Como entrada, é necessário um modelo PyTorch, um dicionário de carregadores de dados, uma função de perda, um otimizador, um número especificado de épocas para treinar e validar e um sinalizador booleano para quando o modelo é um modelo Inception. O sinalizador is_inception é usado para acomodar o modelo Inception v3, já que essa arquitetura usa uma saída auxiliar e a perda geral do modelo respeita a saída auxiliar e a saída final, conforme descrito aqui. A função treina para o número especificado de épocas e, após cada época, executa uma etapa de validação completa. Ele também controla o modelo de melhor desempenho (em termos de precisão de validação) e, no final do treinamento, retorna o modelo de melhor desempenho. Após cada época, as precisões de treinamento e validação são impressas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6XE-MRuhstq"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCvPcZQdhstt"
      },
      "source": [
        "Definir o atributo .requires_grad dos parâmetros do modelo\n",
        "----------------------------------------------\n",
        "\n",
        "Esta função auxiliar define o atributo `` .requires_grad`` do\n",
        "parâmetros no modelo para False quando estamos extraindo recursos. Por\n",
        "padrão, quando carregamos um modelo pré-treinado, todos os parâmetros têm\n",
        "`` .requires_grad = True``, o que é bom se estivermos treinando do zero\n",
        "ou ajuste fino. No entanto, se estamos extraindo recursos e apenas queremos\n",
        "computar gradientes para a camada recém-inicializada, então queremos todos\n",
        "os outros parâmetros não requerem gradientes. Isso vai fazer mais sentido\n",
        "mais tarde.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iILX3gNchstv"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLNTXVRbhsty"
      },
      "source": [
        "Inicializar e remodelar as redes\n",
        "-----------------------------------\n",
        "\n",
        "Agora, para a parte mais interessante. Aqui é onde lidamos com a remodelação\n",
        "de cada rede. Observe, este não é um procedimento automático e é único\n",
        "para cada modelo. Lembre-se, a camada final de um modelo CNN, que muitas vezes é\n",
        "vezes uma camada FC, tem o mesmo número de nós que o número de saída\n",
        "classes no conjunto de dados. Uma vez que todos os modelos foram pré-treinados em\n",
        "Imagenet, todos eles têm camadas de saída de tamanho 1000, um nó para cada\n",
        "classe. O objetivo aqui é remodelar a última camada para ter o mesmo\n",
        "número de entradas como antes, E para ter o mesmo número de saídas que\n",
        "o número de classes no conjunto de dados. Nas seções a seguir, iremos\n",
        "discuta como alterar a arquitetura de cada modelo individualmente. Mas\n",
        "primeiro, há um detalhe importante a respeito da diferença entre\n",
        "ajuste fino e extração de recursos.\n",
        "\n",
        "Ao extrair recursos, queremos apenas atualizar os parâmetros do\n",
        "última camada, ou em outras palavras, queremos apenas atualizar os parâmetros para\n",
        "a (s) camada (s) que estamos remodelando. Portanto, não precisamos calcular o\n",
        "gradientes dos parâmetros que não estamos alterando, portanto, para eficiência\n",
        "definimos o atributo .requires_grad como False. Isso é importante porque\n",
        "por padrão, este atributo é definido como True. Então, quando inicializamos o\n",
        "nova camada e por padrão os novos parâmetros têm `` .requires_grad = True``\n",
        "portanto, apenas os parâmetros da nova camada serão atualizados. Quando estamos\n",
        "no ajuste fino, podemos deixar todos os .required_grad configurados para o padrão\n",
        "de verdade.\n",
        "\n",
        "Finalmente, observe que inception_v3 requer que o tamanho da entrada seja\n",
        "(299.299), enquanto todos os outros modelos esperam (224.224).\n",
        "\n",
        "Resnet\n",
        "------\n",
        "\n",
        "Resnet foi apresentado no artigo [Deep Residual Learning for Image\n",
        "Reconhecimento] (https://arxiv.org/abs/1512.03385). Existem vários\n",
        "variantes de tamanhos diferentes, incluindo Resnet18, Resnet34, Resnet50,\n",
        "Resnet101 e Resnet152, todos disponíveis em torchvision\n",
        "modelos. Aqui usamos Resnet18, pois nosso conjunto de dados é pequeno e tem apenas dois\n",
        "Aulas. Quando imprimimos o modelo, vemos que a última camada é totalmente\n",
        "camada conectada conforme mostrado abaixo:\n",
        "\n",
        "    (fc): Linear (in_features = 512, out_features = 1000, bias = True)\n",
        "\n",
        "Assim, devemos reinicializar `` model.fc`` para ser uma camada linear com 512\n",
        "recursos de entrada e 2 recursos de saída com:\n",
        "\n",
        "    model.fc = nn.Linear (512, num_classes)\n",
        "\n",
        "Alexnet\n",
        "-------\n",
        "\n",
        "Alexnet foi apresentado no artigo [ImageNet Classification with Deep\n",
        "Neural convolucional\n",
        "Redes] (https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
        "e foi a primeira CNN muito bem-sucedida no conjunto de dados ImageNet. Quando nós\n",
        "imprimir a arquitetura do modelo, vemos que a saída do modelo vem do 6º\n",
        "camada do classificador\n",
        "\n",
        "    (classificador): Sequencial (\n",
        "       ...\n",
        "       (6): Linear (in_features = 4096, out_features = 1000, bias = True)\n",
        "    )\n",
        "\n",
        "Para usar o modelo com nosso conjunto de dados, reinicializamos esta camada como\n",
        "\n",
        "    model.classifier [6] = nn.Linear (4096, num_classes)\n",
        "\n",
        "VGG\n",
        "---\n",
        "\n",
        "VGG foi apresentado no artigo [Very Deep Convolutional Networks for\n",
        "Reconhecimento de imagem em grande escala] (https://arxiv.org/pdf/1409.1556.pdf).\n",
        "A Torchvision oferece oito versões do VGG com vários comprimentos e alguns\n",
        "que possuem camadas de normalização em lote. Aqui usamos VGG-11 com lote\n",
        "normalização. A camada de saída é semelhante a Alexnet, ou seja,\n",
        "\n",
        "    (classificador): Sequencial (\n",
        "       ...\n",
        "       (6): Linear (in_features = 4096, out_features = 1000, bias = True)\n",
        "    )\n",
        "\n",
        "Portanto, usamos a mesma técnica para modificar a camada de saída\n",
        "\n",
        "    model.classifier [6] = nn.Linear (4096, num_classes)\n",
        "\n",
        "Squeezenet\n",
        "----------\n",
        "\n",
        "A arquitetura Squeeznet é descrita no artigo [SqueezeNet:\n",
        "Precisão no nível do AlexNet com 50x menos parâmetros e tamanho do modelo inferior a 0,5 MB] (https://arxiv.org/abs/1602.07360) e usa uma saída diferente\n",
        "estrutura do que qualquer um dos outros modelos mostrados aqui. Torchvision tem dois\n",
        "versões do Squeezenet, usamos a versão 1.0. A saída vem de um 1x1\n",
        "camada convolucional que é a 1ª camada do classificador:\n",
        "\n",
        "    (classificador): Sequencial (\n",
        "       (0): Desistência (p = 0,5)\n",
        "       (1): Conv2d (512, 1000, kernel_size = (1, 1), passo = (1, 1))\n",
        "       (2): ReLU (no local)\n",
        "       (3): AvgPool2d (kernel_size = 13, passo = 1, preenchimento = 0)\n",
        "    )\n",
        "\n",
        "Para modificar a rede, reinicializamos a camada Conv2d para ter um\n",
        "mapa de recursos de saída de profundidade 2 como\n",
        "\n",
        "    model.classifier [1] = nn.Conv2d (512, num_classes, kernel_size = (1,1), passo = (1,1))\n",
        "\n",
        "Densenet\n",
        "--------\n",
        "\n",
        "Densenet foi apresentado no artigo [Densely Connected Convolutional\n",
        "Networks] (https://arxiv.org/abs/1608.06993). Torchvision tem quatro\n",
        "variantes de Densenet, mas aqui usamos apenas Densenet-121. A camada de saída\n",
        "é uma camada linear com 1024 recursos de entrada:\n",
        "\n",
        "    (classificador): Linear (in_features = 1024, out_features = 1000, bias = True)\n",
        "\n",
        "Para remodelar a rede, reinicializamos a camada linear do classificador como\n",
        "\n",
        "    model.classifier = nn.Linear (1024, num_classes)\n",
        "\n",
        "Inception v3\n",
        "------------\n",
        "\n",
        "Finalmente, o Inception v3 foi descrito pela primeira vez em [Repensando o Inception\n",
        "Arquitetura para Computador\n",
        "Visão] (https://arxiv.org/pdf/1512.00567v1.pdf). Esta rede é\n",
        "exclusivo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WEANIHZhstz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "79731c780f194ed7827d9eccd35c0aaf",
            "e69d6da97b23426295bf5f8dc1c9a80d",
            "082051845fd14a75a14961e05606d8f5",
            "cf9cdb28b9ee44f2a87afb9dd414cb77",
            "2ec5eb70fa284210acc8b828dd6fd394",
            "8ac63f248527417d8c7a3b21669b727e",
            "dd0473e7a5294cd0a6d7bca55e29142a",
            "7d73b24850a8451892d594b3ed34eabe",
            "ca3f8de0388445feae00f2b0f50759d0",
            "a847ae59e16542e9a195ef7a127bb8cf",
            "28c6b88904c445c78ee2319926dc7c5b"
          ]
        },
        "outputId": "6eacac24-4d48-43b8-9016-9f88db06f38f"
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3 \n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79731c780f194ed7827d9eccd35c0aaf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/4.78M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a62V-1hBhst2"
      },
      "source": [
        "Ler os dados\n",
        "---------\n",
        "\n",
        "Agora que sabemos qual deve ser o tamanho da entrada, podemos inicializar as transformações de dados, os conjuntos de dados de imagens e os carregadores de dados. Observe que os modelos foram pré-treinados com os valores de normalização embutidos em código.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_ysIETniX9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e8fb5e-732a-44c8-954a-567f8352acdb"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
        "!unzip hymenoptera_data.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-27 00:48:07--  https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.249.93.14, 13.249.93.46, 13.249.93.56, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.249.93.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47286322 (45M) [application/zip]\n",
            "Saving to: ‘hymenoptera_data.zip’\n",
            "\n",
            "hymenoptera_data.zi 100%[===================>]  45.10M  48.5MB/s    in 0.9s    \n",
            "\n",
            "2021-11-27 00:48:09 (48.5 MB/s) - ‘hymenoptera_data.zip’ saved [47286322/47286322]\n",
            "\n",
            "Archive:  hymenoptera_data.zip\n",
            "   creating: hymenoptera_data/\n",
            "   creating: hymenoptera_data/train/\n",
            "   creating: hymenoptera_data/train/ants/\n",
            "  inflating: hymenoptera_data/train/ants/0013035.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1030023514_aad5c608f9.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1095476100_3906d8afde.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1099452230_d1949d3250.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/116570827_e9c126745d.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1225872729_6f0856588f.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1262877379_64fcada201.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1269756697_0bce92cdab.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1286984635_5119e80de1.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/132478121_2a430adea2.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1360291657_dc248c5eea.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1368913450_e146e2fb6d.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1473187633_63ccaacea6.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/148715752_302c84f5a4.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1489674356_09d48dde0a.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/149244013_c529578289.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/150801003_3390b73135.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/150801171_cd86f17ed8.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/154124431_65460430f2.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/162603798_40b51f1654.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1660097129_384bf54490.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/167890289_dd5ba923f3.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1693954099_46d4c20605.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/175998972.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/178538489_bec7649292.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1804095607_0341701e1c.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1808777855_2a895621d7.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/188552436_605cc9b36b.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1917341202_d00a7f9af5.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/1924473702_daa9aacdbe.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/196057951_63bf063b92.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/196757565_326437f5fe.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/201558278_fe4caecc76.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/201790779_527f4c0168.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/2019439677_2db655d361.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/207947948_3ab29d7207.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/20935278_9190345f6b.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/224655713_3956f7d39a.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/2265824718_2c96f485da.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/2265825502_fff99cfd2d.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/226951206_d6bf946504.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/2278278459_6b99605e50.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/2288450226_a6e96e8fdf.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/2288481644_83ff7e4572.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/2292213964_ca51ce4bef.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/24335309_c5ea483bb8.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/245647475_9523dfd13e.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/255434217_1b2b3fe0a4.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/258217966_d9d90d18d3.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/275429470_b2d7d9290b.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/28847243_e79fe052cd.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/318052216_84dff3f98a.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/334167043_cbd1adaeb9.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/339670531_94b75ae47a.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/342438950_a3da61deab.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/36439863_0bec9f554f.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/374435068_7eee412ec4.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/382971067_0bfd33afe0.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/384191229_5779cf591b.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/386190770_672743c9a7.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/392382602_1b7bed32fa.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/403746349_71384f5b58.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/408393566_b5b694119b.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/424119020_6d57481dab.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/424873399_47658a91fb.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/450057712_771b3bfc91.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/45472593_bfd624f8dc.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/459694881_ac657d3187.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/460372577_f2f6a8c9fc.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/460874319_0a45ab4d05.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/466430434_4000737de9.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/470127037_513711fd21.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/474806473_ca6caab245.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/475961153_b8c13fd405.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/484293231_e53cfc0c89.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/49375974_e28ba6f17e.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/506249802_207cd979b4.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/506249836_717b73f540.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/512164029_c0a66b8498.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/512863248_43c8ce579b.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/518773929_734dbc5ff4.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/522163566_fec115ca66.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/522415432_2218f34bf8.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/531979952_bde12b3bc0.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/533848102_70a85ad6dd.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/535522953_308353a07c.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/540889389_48bb588b21.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/541630764_dbd285d63c.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/543417860_b14237f569.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/560966032_988f4d7bc4.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/5650366_e22b7e1065.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/6240329_72c01e663e.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/6240338_93729615ec.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/649026570_e58656104b.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/662541407_ff8db781e7.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/67270775_e9fdf77e9d.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/6743948_2b8c096dda.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/684133190_35b62c0c1d.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/69639610_95e0de17aa.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/707895295_009cf23188.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/7759525_1363d24e88.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/795000156_a9900a4a71.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/822537660_caf4ba5514.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/82852639_52b7f7f5e3.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/841049277_b28e58ad05.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/886401651_f878e888cd.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/892108839_f1aad4ca46.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/938946700_ca1c669085.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/957233405_25c1d1187b.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/9715481_b3cb4114ff.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/998118368_6ac1d91f81.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/ant photos.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/Ant_1.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/army-ants-red-picture.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/formica.jpeg  \n",
            "  inflating: hymenoptera_data/train/ants/hormiga_co_por.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/imageNotFound.gif  \n",
            "  inflating: hymenoptera_data/train/ants/kurokusa.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/MehdiabadiAnt2_600.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/Nepenthes_rafflesiana_ant.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/swiss-army-ant.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/termite-vs-ant.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/trap-jaw-ant-insect-bg.jpg  \n",
            "  inflating: hymenoptera_data/train/ants/VietnameseAntMimicSpider.jpg  \n",
            "   creating: hymenoptera_data/train/bees/\n",
            "  inflating: hymenoptera_data/train/bees/1092977343_cb42b38d62.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/1093831624_fb5fbe2308.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/1097045929_1753d1c765.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/1232245714_f862fbe385.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/129236073_0985e91c7d.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/1295655112_7813f37d21.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/132511197_0b86ad0fff.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/132826773_dbbcb117b9.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/150013791_969d9a968b.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/1508176360_2972117c9d.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/154600396_53e1252e52.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/16838648_415acd9e3f.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/1691282715_0addfdf5e8.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/17209602_fe5a5a746f.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/174142798_e5ad6d76e0.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/1799726602_8580867f71.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/1807583459_4fe92b3133.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/196430254_46bd129ae7.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/196658222_3fffd79c67.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/198508668_97d818b6c4.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2031225713_50ed499635.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2037437624_2d7bce461f.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2053200300_8911ef438a.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/205835650_e6f2614bee.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/208702903_42fb4d9748.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/21399619_3e61e5bb6f.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2227611847_ec72d40403.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2321139806_d73d899e66.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2330918208_8074770c20.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2345177635_caf07159b3.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2358061370_9daabbd9ac.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2364597044_3c3e3fc391.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2384149906_2cd8b0b699.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2397446847_04ef3cd3e1.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2405441001_b06c36fa72.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2445215254_51698ff797.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2452236943_255bfd9e58.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2467959963_a7831e9ff0.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2470492904_837e97800d.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2477324698_3d4b1b1cab.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2477349551_e75c97cf4d.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2486729079_62df0920be.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2486746709_c43cec0e42.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2493379287_4100e1dacc.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2495722465_879acf9d85.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2528444139_fa728b0f5b.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2538361678_9da84b77e3.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2551813042_8a070aeb2b.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2580598377_a4caecdb54.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2601176055_8464e6aa71.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2610833167_79bf0bcae5.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2610838525_fe8e3cae47.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2617161745_fa3ebe85b4.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2625499656_e3415e374d.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2634617358_f32fd16bea.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2638074627_6b3ae746a0.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2645107662_b73a8595cc.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2651621464_a2fa8722eb.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2652877533_a564830cbf.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/266644509_d30bb16a1b.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2683605182_9d2a0c66cf.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2704348794_eb5d5178c2.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2707440199_cd170bd512.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2710368626_cb42882dc8.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2722592222_258d473e17.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2728759455_ce9bb8cd7a.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2756397428_1d82a08807.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2765347790_da6cf6cb40.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2781170484_5d61835d63.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/279113587_b4843db199.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2792000093_e8ae0718cf.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2801728106_833798c909.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2822388965_f6dca2a275.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2861002136_52c7c6f708.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2908916142_a7ac8b57a8.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/29494643_e3410f0d37.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2959730355_416a18c63c.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/2962405283_22718d9617.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/3006264892_30e9cced70.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/3030189811_01d095b793.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/3030772428_8578335616.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/3044402684_3853071a87.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/3074585407_9854eb3153.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/3079610310_ac2d0ae7bc.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/3090975720_71f12e6de4.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/3100226504_c0d4f1e3f1.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/342758693_c56b89b6b6.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/354167719_22dca13752.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/359928878_b3b418c728.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/365759866_b15700c59b.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/36900412_92b81831ad.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/39672681_1302d204d1.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/39747887_42df2855ee.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/421515404_e87569fd8b.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/444532809_9e931e2279.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/446296270_d9e8b93ecf.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/452462677_7be43af8ff.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/452462695_40a4e5b559.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/457457145_5f86eb7e9c.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/465133211_80e0c27f60.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/469333327_358ba8fe8a.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/472288710_2abee16fa0.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/473618094_8ffdcab215.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/476347960_52edd72b06.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/478701318_bbd5e557b8.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/507288830_f46e8d4cb2.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/509247772_2db2d01374.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/513545352_fd3e7c7c5d.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/522104315_5d3cb2758e.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/537309131_532bfa59ea.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/586041248_3032e277a9.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/760526046_547e8b381f.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/760568592_45a52c847f.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/774440991_63a4aa0cbe.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/85112639_6e860b0469.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/873076652_eb098dab2d.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/90179376_abc234e5f4.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/92663402_37f379e57a.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/95238259_98470c5b10.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/969455125_58c797ef17.jpg  \n",
            "  inflating: hymenoptera_data/train/bees/98391118_bdb1e80cce.jpg  \n",
            "   creating: hymenoptera_data/val/\n",
            "   creating: hymenoptera_data/val/ants/\n",
            "  inflating: hymenoptera_data/val/ants/10308379_1b6c72e180.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1053149811_f62a3410d3.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1073564163_225a64f170.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1119630822_cd325ea21a.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1124525276_816a07c17f.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/11381045_b352a47d8c.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/119785936_dd428e40c3.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1247887232_edcb61246c.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1262751255_c56c042b7b.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1337725712_2eb53cd742.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1358854066_5ad8015f7f.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1440002809_b268d9a66a.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/147542264_79506478c2.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/152286280_411648ec27.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/153320619_2aeb5fa0ee.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/153783656_85f9c3ac70.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/157401988_d0564a9d02.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/159515240_d5981e20d1.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/161076144_124db762d6.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/161292361_c16e0bf57a.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/170652283_ecdaff5d1a.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/17081114_79b9a27724.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/172772109_d0a8e15fb0.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/1743840368_b5ccda82b7.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/181942028_961261ef48.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/183260961_64ab754c97.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/2039585088_c6f47c592e.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/205398178_c395c5e460.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/208072188_f293096296.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/209615353_eeb38ba204.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/2104709400_8831b4fc6f.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/212100470_b485e7b7b9.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/2127908701_d49dc83c97.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/2191997003_379df31291.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/2211974567_ee4606b493.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/2219621907_47bc7cc6b0.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/2238242353_52c82441df.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/2255445811_dabcdf7258.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/239161491_86ac23b0a3.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/263615709_cfb28f6b8e.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/308196310_1db5ffa01b.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/319494379_648fb5a1c6.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/35558229_1fa4608a7a.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/412436937_4c2378efc2.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/436944325_d4925a38c7.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/445356866_6cb3289067.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/459442412_412fecf3fe.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/470127071_8b8ee2bd74.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/477437164_bc3e6e594a.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/488272201_c5aa281348.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/502717153_3e4865621a.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/518746016_bcc28f8b5b.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/540543309_ddbb193ee5.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/562589509_7e55469b97.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/57264437_a19006872f.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/573151833_ebbc274b77.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/649407494_9b6bc4949f.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/751649788_78dd7d16ce.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/768870506_8f115d3d37.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/800px-Meat_eater_ant_qeen_excavating_hole.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/8124241_36b290d372.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/8398478_50ef10c47a.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/854534770_31f6156383.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/892676922_4ab37dce07.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/94999827_36895faade.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/Ant-1818.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/ants-devouring-remains-of-large-dead-insect-on-red-tile-in-Stellenbosch-South-Africa-closeup-1-DHD.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/desert_ant.jpg  \n",
            "  inflating: hymenoptera_data/val/ants/F.pergan.28(f).jpg  \n",
            "  inflating: hymenoptera_data/val/ants/Hormiga.jpg  \n",
            "   creating: hymenoptera_data/val/bees/\n",
            "  inflating: hymenoptera_data/val/bees/1032546534_06907fe3b3.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/10870992_eebeeb3a12.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/1181173278_23c36fac71.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/1297972485_33266a18d9.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/1328423762_f7a88a8451.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/1355974687_1341c1face.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/144098310_a4176fd54d.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/1486120850_490388f84b.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/149973093_da3c446268.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/151594775_ee7dc17b60.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/151603988_2c6f7d14c7.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/1519368889_4270261ee3.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/152789693_220b003452.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/177677657_a38c97e572.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/1799729694_0c40101071.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/181171681_c5a1a82ded.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/187130242_4593a4c610.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/203868383_0fcbb48278.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2060668999_e11edb10d0.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2086294791_6f3789d8a6.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2103637821_8d26ee6b90.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2104135106_a65eede1de.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/215512424_687e1e0821.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2173503984_9c6aaaa7e2.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/220376539_20567395d8.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/224841383_d050f5f510.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2321144482_f3785ba7b2.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/238161922_55fa9a76ae.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2407809945_fb525ef54d.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2415414155_1916f03b42.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2438480600_40a1249879.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2444778727_4b781ac424.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2457841282_7867f16639.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2470492902_3572c90f75.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2478216347_535c8fe6d7.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2501530886_e20952b97d.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2506114833_90a41c5267.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2509402554_31821cb0b6.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2525379273_dcb26a516d.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/26589803_5ba7000313.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2668391343_45e272cd07.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2670536155_c170f49cd0.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2685605303_9eed79d59d.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2702408468_d9ed795f4f.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2709775832_85b4b50a57.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2717418782_bd83307d9f.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/272986700_d4d4bf8c4b.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2741763055_9a7bb00802.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2745389517_250a397f31.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2751836205_6f7b5eff30.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2782079948_8d4e94a826.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2809496124_5f25b5946a.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2815838190_0a9889d995.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2841437312_789699c740.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/2883093452_7e3a1eb53f.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/290082189_f66cb80bfc.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/296565463_d07a7bed96.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/3077452620_548c79fda0.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/348291597_ee836fbb1a.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/350436573_41f4ecb6c8.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/353266603_d3eac7e9a0.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/372228424_16da1f8884.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/400262091_701c00031c.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/416144384_961c326481.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/44105569_16720a960c.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/456097971_860949c4fc.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/464594019_1b24a28bb1.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/485743562_d8cc6b8f73.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/540976476_844950623f.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/54736755_c057723f64.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/57459255_752774f1b2.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/576452297_897023f002.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/586474709_ae436da045.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/590318879_68cf112861.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/59798110_2b6a3c8031.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/603709866_a97c7cfc72.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/603711658_4c8cd2201e.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/65038344_52a45d090d.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/6a00d8341c630a53ef00e553d0beb18834-800wi.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/72100438_73de9f17af.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/759745145_e8bc776ec8.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/936182217_c4caa5222d.jpg  \n",
            "  inflating: hymenoptera_data/val/bees/abeja.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5h5t-00hst3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c14176-c8d0-46bc-fbbe-785987836225"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L09eyyfUhst8"
      },
      "source": [
        "Cria o Otimizador\n",
        "--------------------\n",
        "\n",
        "Agora que a estrutura do modelo está correta, a etapa final para o ajuste fino\n",
        "e a extração de recursos é criar um otimizador que apenas atualiza o\n",
        "parâmetros desejados. Lembre-se que depois de carregar o modelo pré-treinado, mas\n",
        "antes de remodelar, se `` feature_extract = True`` nós definimos manualmente todos os\n",
        "atributos `` .requires_grad`` do parâmetro para False. Então o\n",
        "os parâmetros da camada reinicializada têm `` .requires_grad = True`` por\n",
        "predefinição. Agora sabemos que * todos os parâmetros que têm\n",
        ".requires_grad = True deve ser otimizado. * A seguir, fazemos uma lista de tais\n",
        "parâmetros e insira esta lista para o construtor do algoritmo SGD.\n",
        "\n",
        "Para verificar isso, verifique os parâmetros impressos para aprender. Quando\n",
        "ajuste fino, esta lista deve ser longa e incluir todo o modelo\n",
        "parâmetros. No entanto, ao extrair recursos, esta lista deve ser curta\n",
        "e inclui apenas os pesos e inclinações das camadas remodeladas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRi1Ma1Thst9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c67a79c-cf26-443b-a95b-e3ccfca5ee16"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are \n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t classifier.1.weight\n",
            "\t classifier.1.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mocmRbo8hsuB"
      },
      "source": [
        "Executar etapa de treinamento e validação\n",
        "--------------------------------\n",
        "\n",
        "Finalmente, a última etapa é configurar a perda para o modelo e, em seguida, executar o\n",
        "função de treinamento e validação para o número definido de épocas. Perceber,\n",
        "dependendo do número de épocas, esta etapa pode demorar um pouco em uma CPU.\n",
        "Além disso, a taxa de aprendizagem padrão não é ideal para todos os modelos, então\n",
        "para atingir a precisão máxima, seria necessário ajustar para cada modelo\n",
        "separadamente.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfl9S6PnhsuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79e5377-37cc-4bcc-ef7b-38a1b8c03192"
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5338 Acc: 0.7213\n",
            "val Loss: 0.4960 Acc: 0.8039\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.2541 Acc: 0.8934\n",
            "val Loss: 0.3553 Acc: 0.8824\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.2997 Acc: 0.8689\n",
            "val Loss: 0.3192 Acc: 0.9020\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.2138 Acc: 0.9221\n",
            "val Loss: 0.3243 Acc: 0.9150\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.2067 Acc: 0.9139\n",
            "val Loss: 0.4091 Acc: 0.8693\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.2002 Acc: 0.9098\n",
            "val Loss: 0.3791 Acc: 0.9020\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.1772 Acc: 0.9180\n",
            "val Loss: 0.3282 Acc: 0.9346\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.1422 Acc: 0.9385\n",
            "val Loss: 0.3315 Acc: 0.9216\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.1342 Acc: 0.9426\n",
            "val Loss: 0.3860 Acc: 0.9085\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.1702 Acc: 0.9180\n",
            "val Loss: 0.3738 Acc: 0.9281\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.1783 Acc: 0.9344\n",
            "val Loss: 0.3176 Acc: 0.9150\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.1214 Acc: 0.9549\n",
            "val Loss: 0.3451 Acc: 0.9216\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.1052 Acc: 0.9631\n",
            "val Loss: 0.4380 Acc: 0.8824\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.1528 Acc: 0.9262\n",
            "val Loss: 0.3704 Acc: 0.9281\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.1354 Acc: 0.9385\n",
            "val Loss: 0.3373 Acc: 0.9216\n",
            "\n",
            "Training complete in 0m 55s\n",
            "Best val Acc: 0.934641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WivROOUBhsuG"
      },
      "source": [
        "Comparação com modelo treinado do zero\n",
        "------------------------------------------\n",
        "\n",
        "Apenas por diversão, vamos ver como o modelo aprende se não usarmos transferência\n",
        "Aprendendo. O desempenho do ajuste fino vs. extração de recursos depende\n",
        "em grande parte no conjunto de dados, mas em geral ambos os métodos de aprendizagem de transferência\n",
        "produzir resultados favoráveis em termos de tempo de treinamento e precisão geral\n",
        "versus um modelo treinado do zero.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBg19vQmhsuG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03eba06a-b0fc-4705-8736-5b5b41fb6e25"
      },
      "source": [
        "# Initialize the non-pretrained version of the model used for this run\n",
        "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
        "scratch_model = scratch_model.to(device)\n",
        "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
        "scratch_criterion = nn.CrossEntropyLoss()\n",
        "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
        "\n",
        "# Plot the training curves of validation accuracy vs. number \n",
        "#  of training epochs for the transfer learning method and\n",
        "#  the model trained from scratch\n",
        "ohist = []\n",
        "shist = []\n",
        "\n",
        "ohist = [h.cpu().numpy() for h in hist]\n",
        "shist = [h.cpu().numpy() for h in scratch_hist]\n",
        "\n",
        "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
        "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
        "plt.ylim((0,1.))\n",
        "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.7066 Acc: 0.4385\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.6932 Acc: 0.4713\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.6932 Acc: 0.5041\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.6931 Acc: 0.5205\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.6931 Acc: 0.4754\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.6930 Acc: 0.5574\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.6932 Acc: 0.5082\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.6930 Acc: 0.5287\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.6930 Acc: 0.5164\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.6930 Acc: 0.5205\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.6929 Acc: 0.5164\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.6928 Acc: 0.5041\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.6917 Acc: 0.5041\n",
            "val Loss: 0.6930 Acc: 0.4575\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.6911 Acc: 0.5041\n",
            "val Loss: 0.6930 Acc: 0.4575\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.6909 Acc: 0.5082\n",
            "val Loss: 0.6931 Acc: 0.4575\n",
            "\n",
            "Training complete in 1m 10s\n",
            "Best val Acc: 0.457516\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}